{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "157e1748",
   "metadata": {},
   "source": [
    "# Using Twitter's Academic API to search for tweets (with geotags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "804d4278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "from ast import literal_eval\n",
    "\n",
    "def unnest_json(dataframe, column):\n",
    "    dataframe_new = json_normalize(dataframe[column].apply(literal_eval))\n",
    "    return dataframe_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c2582ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 tweets saved in the file\n",
      "998 tweets saved in the file\n",
      "1492 tweets saved in the file\n",
      "1984 tweets saved in the file\n",
      "2469 tweets saved in the file\n",
      "2950 tweets saved in the file\n",
      "3422 tweets saved in the file\n",
      "3899 tweets saved in the file\n",
      "Finished searching for tweets!Total Tweet IDs saved: 3899\n"
     ]
    }
   ],
   "source": [
    "# Accessing the Academic API \n",
    "def connect_to_endpoint(bearer_token, query, next_token=None):\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    # add additional parameters as needed\n",
    "    params = {\n",
    "        'expansions' : \"author_id,referenced_tweets.id,geo.place_id,in_reply_to_user_id,referenced_tweets.id.author_id\",\n",
    "        'tweet.fields' : \"attachments,author_id,context_annotations,created_at,entities,public_metrics\",\n",
    "        'user.fields' : \"created_at,username,verified,description,entities,id,location,name,public_metrics,url\",\n",
    "        'place.fields' : \"contained_within,country,country_code,full_name,geo,id,name,place_type\"}\n",
    "    # replace appropriate start and end times below, in our study, we collected tweets date back to 2013\n",
    "    if (next_token is not None):\n",
    "        url = \"https://api.twitter.com/2/tweets/search/all?max_results=500&query={}&start_time=2013-01-01T00:00:00Z&end_time=2021-03-31T23:59:59.000Z&next_token={}\".format(query, next_token)\n",
    "    else:\n",
    "        url = \"https://api.twitter.com/2/tweets/search/all?max_results=500&start_time=2013-01-01T00:00:00Z&end_time=2021-03-31T23:59:59.000Z&query={}\".format(query)\n",
    "    response = requests.request(\"GET\", url, params=params, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()\n",
    "count = 0\n",
    "result_count = 0\n",
    "flag = True\n",
    "# Replace with your own bearer token from your academic project in developer portal\n",
    "bearer_token = \"XXXX\"\n",
    "tmp = pd.DataFrame()\n",
    "while flag:\n",
    "    # Replace the count below with the number of Tweets you want to stop at. \n",
    "    # Note: running without the count check will result in getting more Tweets\n",
    "    # that will count towards the Tweet cap\n",
    "    if count >= 7000000:\n",
    "        break\n",
    "    json_response = connect_to_endpoint(bearer_token, '(\"belt and road\" OR \"one belt one road\" OR \"new silk road\" OR %23beltandroad OR %23beltandroadinitiative OR %23cpec OR %23obor) lang:en -is:retweet -is:reply has:geo')\n",
    "    while 'next_token' in json_response['meta']:\n",
    "        next_token = json_response['meta']['next_token']\n",
    "        result_count = json_response['meta']['result_count']\n",
    "        #print(next_token)\n",
    "        if result_count is not None and result_count > 0 and next_token is not None:\n",
    "            df_tweet = pd.DataFrame(json_response['data'])\n",
    "            df_user = pd.DataFrame(json_response['includes']['users'])\n",
    "            df_places = pd.DataFrame(json_response['includes']['places'])\n",
    "            df_full = pd.merge(df_tweet, df_user, how = 'left', left_on = 'author_id', right_on = 'id', suffixes=('_tweet', '_user'))\n",
    "            df_full['geo'] = df_full['geo'].fillna({i: {} for i in df_full.index})\n",
    "            df_full['geo_id'] =  df_full['geo'].apply(lambda x: x.get('place_id'))\n",
    "            df_full = pd.merge(df_full, df_places, how = 'outer', left_on = 'geo_id', right_on = 'id', suffixes=('_meta', '_geo'))\n",
    "            tmp = tmp.append(df_full)\n",
    "            count += result_count\n",
    "            print(\"{} tweets saved in the file\".format(count))\n",
    "            json_response = connect_to_endpoint(bearer_token, '(\"belt and road\" OR \"one belt one road\" OR \"new silk road\" OR %23beltandroad OR %23beltandroadinitiative OR %23cpec OR %23obor) lang:en -is:retweet -is:reply has:geo', next_token)\n",
    "    else:\n",
    "        flag = False\n",
    "print(\"Finished searching for tweets!Total Tweet IDs saved: {}\".format(count))\n",
    "tmp.to_csv(\"tweets.csv\", encoding=\"utf-8\",index=False, escapechar=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1525e2f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3899"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b76468",
   "metadata": {},
   "source": [
    "## Finding top ranked hashtags in the tweets. This step could also help extend the list of keywords used to collect tweets (building up the queries by adding top-ranked hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf712495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import advertools\n",
    "text = tmp.text.values.tolist()\n",
    "hashtag_summary = advertools.extract_hashtags(text)\n",
    "top_hashtags = hashtag_summary['top_hashtags'][0:20]\n",
    "top_hashtags"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
